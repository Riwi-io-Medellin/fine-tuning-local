instruction,response
"¿Qué es fine-tuning de un modelo de lenguaje?","Fine-tuning es el proceso de especializar un modelo de lenguaje pre-entrenado para una tarea o dominio específico. En lugar de entrenar desde cero, tomamos un modelo que ya aprendió patrones generales del lenguaje y lo adaptamos con datos más específicos. Esto ahorra tiempo, recursos computacionales y produce mejores resultados que entrenar desde cero."
"Explica la diferencia entre prompting y fine-tuning","Prompting es dar instrucciones al modelo en tiempo de inferencia sin modificar sus pesos. Fine-tuning modifica los pesos del modelo entrenándolo con nuevos datos. Prompting es más flexible y no requiere entrenamiento, pero fine-tuning produce respuestas más consistentes y especializadas cuando tienes suficientes datos del dominio."
"¿Qué es LoRA y por qué se usa?","LoRA (Low-Rank Adaptation) es una técnica que permite fine-tunear modelos grandes entrenando solo matrices pequeñas en lugar de todos los parámetros. Reduce drásticamente la memoria necesaria (de GBs a MBs) y el tiempo de entrenamiento, mientras mantiene la calidad del modelo original. Es el estándar actual para fine-tuning eficiente."
"¿Qué es QLoRA?","QLoRA combina cuantización en 4-bit con LoRA. El modelo base se carga cuantizado (ocupando menos memoria) y solo se entrenan los adaptadores LoRA. Esto permite fine-tunear modelos de 7B-13B parámetros en GPUs con 12GB de VRAM, democratizando el acceso al fine-tuning."
"¿Cuántos datos necesito para fine-tuning?","Depende del objetivo. Para tareas simples de estilo o formato, 100-500 ejemplos pueden ser suficientes. Para conocimiento de dominio, 1000-5000 ejemplos son recomendables. La calidad importa más que la cantidad: datos limpios y coherentes producen mejores resultados que miles de ejemplos ruidosos."
"¿Qué formato debe tener mi dataset?","Para modelos Instruct, el formato ideal es pares de instrucción-respuesta. Un CSV simple con columnas 'instruction' y 'response' funciona bien. Cada fila representa un ejemplo de cómo quieres que el modelo responda. Evita respuestas demasiado cortas o inconsistentes en estilo."
"¿Qué es un epoch en entrenamiento?","Un epoch es una pasada completa por todo el dataset de entrenamiento. Si tienes 1000 ejemplos y entrenas por 3 epochs, el modelo verá cada ejemplo 3 veces. Más epochs no siempre es mejor: demasiados pueden causar overfitting donde el modelo memoriza en lugar de generalizar."
"¿Qué es overfitting y cómo evitarlo?","Overfitting ocurre cuando el modelo memoriza los datos de entrenamiento en lugar de aprender patrones generales. Se manifiesta como respuestas que repiten exactamente el dataset pero fallan con inputs nuevos. Para evitarlo: usa datasets diversos, limita el número de epochs, y evalúa con datos que el modelo no ha visto."
"¿Qué es GGUF?","GGUF (GPT-Generated Unified Format) es un formato de archivo para modelos de lenguaje optimizado para inferencia. Es el sucesor de GGML y es el formato estándar para llama.cpp y Ollama. Soporta cuantización eficiente y permite ejecutar modelos grandes en hardware modesto."
"¿Qué niveles de cuantización existen en GGUF?","Los principales son: Q4_K_M (buena relación calidad/tamaño), Q5_K_M (mejor calidad, más grande), Q8_0 (alta calidad, cercano a FP16), y F16 (sin cuantización). Q4_K_M es el estándar para uso general, Q5_K_M para cuando necesitas más precisión, y Q8_0 cuando tienes suficiente memoria."
"¿Cómo funciona Ollama internamente?","Ollama usa llama.cpp como backend de inferencia. Gestiona modelos GGUF, maneja el contexto de conversación, y expone una API REST simple. Cuando ejecutas un modelo, Ollama lo carga en memoria, procesa el prompt con el template definido, y genera tokens de forma autoregresiva."
"¿Qué es un Modelfile en Ollama?","Un Modelfile es un archivo de configuración que define cómo Ollama debe usar un modelo. Especifica el archivo GGUF base, el template de prompt (cómo formatear las conversaciones), parámetros como temperatura y contexto máximo, y opcionalmente un mensaje de sistema por defecto."
"¿Qué es el learning rate y cómo elegirlo?","El learning rate determina qué tan grandes son los ajustes a los pesos del modelo en cada paso de entrenamiento. Un valor muy alto causa inestabilidad, muy bajo hace el entrenamiento lento. Para LoRA, valores entre 1e-4 y 2e-4 suelen funcionar bien. Es uno de los hiperparámetros más importantes."
"¿Qué es gradient accumulation?","Gradient accumulation permite simular batch sizes más grandes acumulando gradientes de múltiples mini-batches antes de actualizar los pesos. Si tu GPU solo permite batch_size=2 pero quieres el efecto de batch_size=8, usas gradient_accumulation_steps=4. Útil cuando tienes memoria GPU limitada."
"¿Qué es batch size y cómo afecta al entrenamiento?","Batch size es el número de ejemplos procesados juntos en cada paso. Batch sizes más grandes dan gradientes más estables pero requieren más memoria. Para fine-tuning con LoRA, batch sizes de 2-4 son comunes en GPUs de consumo, compensando con gradient accumulation."
"¿Cómo sé si mi modelo está aprendiendo?","Observa la pérdida (loss) durante el entrenamiento: debe disminuir gradualmente. Si oscila mucho, el learning rate puede ser muy alto. Si no baja, puede ser muy bajo o los datos tienen problemas. También prueba el modelo con prompts fuera del dataset para verificar generalización."
"¿Puedo usar fine-tuning para inyectar conocimiento nuevo?","Fine-tuning es mejor para cambiar el estilo, formato o comportamiento del modelo que para agregar conocimiento factual nuevo. Para conocimiento nuevo, RAG (Retrieval Augmented Generation) suele ser más efectivo. Fine-tuning puede reforzar patrones de información que ya existen parcialmente en el modelo."
"¿Qué es el contexto en un LLM?","El contexto es la cantidad máxima de tokens que el modelo puede considerar al generar una respuesta. Modelos como Llama-3 tienen contextos de 8K-128K tokens. Más contexto permite conversaciones más largas y procesar documentos más extensos, pero usa más memoria."
"¿Qué son los target modules en LoRA?","Target modules son las capas del modelo donde se aplican los adaptadores LoRA. Típicamente son las capas de atención (q_proj, k_proj, v_proj, o_proj) y las capas feed-forward (gate_proj, up_proj, down_proj). Entrenar más módulos da más capacidad de adaptación pero usa más memoria."
"¿Qué es el rank (r) en LoRA?","El rank determina el tamaño de las matrices LoRA. Ranks más altos (32, 64) permiten capturar adaptaciones más complejas pero usan más memoria. Ranks bajos (8, 16) son más eficientes. Para la mayoría de tareas, rank 16 es un buen punto de partida."
"¿Cómo preparo datos desde conversaciones reales?","Extrae pares de pregunta-respuesta limpios. Elimina información personal, normaliza el formato, y asegúrate de que cada respuesta represente cómo quieres que el modelo responda. Descarta ejemplos con respuestas ambiguas o incompletas. La consistencia en estilo es clave."
"¿Qué es un warmup en entrenamiento?","Warmup es un período inicial donde el learning rate aumenta gradualmente desde un valor muy bajo hasta el objetivo. Esto estabiliza el entrenamiento al inicio cuando los gradientes pueden ser inestables. Un warmup ratio de 0.03-0.05 (3-5% del entrenamiento total) es común."
"¿Cuál es la diferencia entre FP16 y BF16?","FP16 (float16) y BF16 (bfloat16) son formatos de precisión reducida. BF16 tiene más rango dinámico y es más estable para entrenamiento, pero requiere hardware compatible (GPUs Ampere+). FP16 es más compatible pero puede tener problemas de overflow. El código debe detectar qué soporta tu GPU."
"¿Qué hacer si el entrenamiento falla por memoria?","Opciones: reduce batch_size a 1 y aumenta gradient_accumulation, usa un modelo más pequeño (1B-3B), reduce max_seq_length, asegúrate de usar cuantización 4-bit. Si nada funciona, considera servicios de GPU en la nube como RunPod o Vast.ai."
"¿Cómo evalúo la calidad de mi modelo fine-tuneado?","Crea un conjunto de test con prompts que el modelo no vio durante entrenamiento. Compara las respuestas con el modelo base. Busca: coherencia, seguimiento de instrucciones, y ausencia de repeticiones o incoherencias. Para tareas específicas, define métricas objetivas cuando sea posible."
